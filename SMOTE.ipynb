{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3xm0Gk2vW2",
        "outputId": "9436be0c-fa79-4096-f424-f49fb0978785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYd2LOTAKGKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91339b1-d999-464b-d451-ebe58d93f234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.3.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sastrawi"
      ],
      "metadata": {
        "id": "XNDuB7CLcNAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf66e31-ce4b-4ef0-e22f-df56b74c4ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/209.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "Ngms3hyOOuTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2d711d-9555-48b9-8f3d-eba157ec1f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "from keras.layers import Bidirectional\n",
        "import keras.layers as layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import Pipeline\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "P0tGMJsXKd2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV"
      ],
      "metadata": {
        "id": "0PiR9fiBjblz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "import re\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import make_scorer"
      ],
      "metadata": {
        "id": "VPXEQ5FrMog0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "from drive.MyDrive.aathesis.func import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "rjk00lHjhWOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('indonesian'))\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "2WHVXWAH7zZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4bc16e-0c23-4de7-a460-07587876908f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = text_to_df('./drive/MyDrive/aathesis/increment_datasets_f2/pc/orig_full.txt')\n",
        "data_df = data_df[:2500]"
      ],
      "metadata": {
        "id": "cn2nDwhhKXEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "  text = BAD_SYMBOLS_RE.sub('', text)\n",
        "  text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "  text = stemmer.stem(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "IKnp3FjVxjOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['text'] = data_df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "PU5rX9Ogv-S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.to_csv(\"./drive/MyDrive/aathesis/data_clean.txt\", sep='\\t', index = None, header = None)"
      ],
      "metadata": {
        "id": "FinkGwKSXMEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = text_to_df('/content/drive/MyDrive/aathesis/increment_datasets_f2/pc/test.txt')\n",
        "test_df['text'] = test_df['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "ViRRVoe_uW1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = text_to_df('/content/drive/MyDrive/aathesis/data_clean.txt')\n",
        "test_df = text_to_df('/content/drive/MyDrive/aathesis/test_clean.txt')"
      ],
      "metadata": {
        "id": "j-AkSLqB4Z4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.drop(data_df[data_df['text'].isna() == True].index, inplace = True)\n",
        "test_df.drop(test_df[test_df['text'].isna() == True].index, inplace = True)"
      ],
      "metadata": {
        "id": "eYuZynXJ7_ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90_hWrOwr3Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TokenizerTransformer(BaseEstimator, TransformerMixin, Tokenizer):\n",
        "\n",
        "    def __init__(self, **tokenizer_params):\n",
        "        Tokenizer.__init__(self, **tokenizer_params)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.fit_on_texts(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = self.texts_to_sequences(X)\n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "O0lcYjBdKhLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "class PadSequencesTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, maxlen):\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_padded = pad_sequences(X, maxlen=self.maxlen)\n",
        "        return X_padded"
      ],
      "metadata": {
        "id": "vEaOFSRHKiw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load GloVe Embedding"
      ],
      "metadata": {
        "id": "L_djo-tY5fV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Copy of idwiki_word2vec_300.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANdoelpA2Mau",
        "outputId": "e3879fb0-554d-41e0-bc66-383d3a665f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Copy of idwiki_word2vec_300.zip\n",
            "  inflating: idwiki_word2vec_300.model.trainables.syn1neg.npy  \n",
            "  inflating: idwiki_word2vec_300.model.wv.vectors.npy  \n",
            "  inflating: idwiki_word2vec_300.model  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv = KeyedVectors.load(\"/content/idwiki_word2vec_300.model\")\n",
        "word_vectors = wv.wv\n",
        "word_vector = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "metadata": {
        "id": "bLfNBtqW2Z30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = datapath('/content/drive/MyDrive/aathesis/glove.6B.300d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ],
      "metadata": {
        "id": "60It-gkU-LrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7575e6c2-2118-4352-cea6-ba0a6fb54ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-2b0ec80d785e>:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_file, word2vec_glove_file)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "metadata": {
        "id": "3b7sBBzaNaVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_vectors) + 1\n",
        "word2vec_len = 300\n",
        "#longest_text = data_df['text'].str.split().str.len().max()\n",
        "sentence_length = 30"
      ],
      "metadata": {
        "id": "khztUNTUMEyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a weight matrix for all words in training text --> size (1800, 300)\n",
        "embedding_matrix = zeros((vocab_size, 300))\n",
        "for word, i in t.word_index.items():\n",
        "  embedding_vector = word_vectors.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    print(word)"
      ],
      "metadata": {
        "id": "9UHcHU7B5xGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = np.vstack([\n",
        "           np.zeros(word_vectors.vectors.shape[1]),\n",
        "           word_vectors.vectors\n",
        "])"
      ],
      "metadata": {
        "id": "sRkHzr7IREmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = list()\n",
        "resample = list()\n",
        "acc_list = list()\n",
        "precision_list = list()\n",
        "recall_list = list()\n",
        "train_length = list()\n",
        "test_length = list()"
      ],
      "metadata": {
        "id": "lKD4KdkVjGwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evals(clf_model, X_test, y_test, train_len, algo=None, sampling=None):\n",
        "  # Test set prediction\n",
        "  #y_prob=clf_model.predict_proba(X_test)\n",
        "  y_pred=clf_model.predict(X_test)\n",
        "  print('Confusion Matrix')\n",
        "  print('='*60)\n",
        "  print(confusion_matrix(y_test,y_pred),\"\\n\")\n",
        "  print('Classification Report')\n",
        "  print('='*60)\n",
        "  print(classification_report(y_test,y_pred),\"\\n\")\n",
        "\n",
        "  models.append(algo)\n",
        "  acc_list.append(accuracy_score(y_test,y_pred))\n",
        "  precision_list.append(precision_score(y_test,y_pred))\n",
        "  recall_list.append(recall_score(y_test,y_pred))\n",
        "  resample.append(sampling)\n",
        "  train_length.append(train_len)\n",
        "  test_length.append(len(X_test))"
      ],
      "metadata": {
        "id": "XudqENUajUrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evals(clf_model, X_test, y_test, train_len, algo=None, sampling=None):\n",
        "  # Test set prediction\n",
        "  loss, accuracy,precision,recall = clf_model.evaluate(X_test,y_test)\n",
        "\n",
        "  models.append(algo)\n",
        "  acc_list.append(accuracy)\n",
        "  precision_list.append(precision)\n",
        "  recall_list.append(recall)\n",
        "  resample.append(sampling)\n",
        "  train_length.append(train_len)\n",
        "  test_length.append(len(X_test))"
      ],
      "metadata": {
        "id": "ljnNTDfjIQNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = TokenizerTransformer()\n",
        "my_padder = PadSequencesTransformer(maxlen=30)\n",
        "my_tokenizer.fit(data_df['text'])\n",
        "tokenized = my_tokenizer.transform(data_df['text'])\n",
        "my_padder.fit(tokenized)\n",
        "X_train = my_padder.transform(tokenized)\n",
        "Y_train = data_df['label']\n",
        "\n",
        "test_tokenizer = TokenizerTransformer()\n",
        "test_padder = PadSequencesTransformer(maxlen=30)\n",
        "test_tokenizer.fit(test_df['text'])\n",
        "test_tokenized = test_tokenizer.transform(test_df['text'])\n",
        "test_padder.fit(test_tokenized)\n",
        "X_test = test_padder.transform(test_tokenized)\n",
        "Y_test = test_df['label']"
      ],
      "metadata": {
        "id": "or8pL2D5sPE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "jq7HlegCwF9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(n_jobs=-1)\n",
        "\n",
        "eval_set = [(X_test, Y_test)]\n",
        "#train model\n",
        "xgb_model.fit(X_train,\n",
        "      Y_train,\n",
        "      verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "Ok7fNS6mwIsm",
        "outputId": "a02175df-e342-454c-c643-e60877dce846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "              num_parallel_tree=None, random_state=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=-1,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_evals(xgb_model, X_test, Y_test, len(data_df['text']), 'XGBoost', 'No Aug')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FRrS9BQwSQx",
        "outputId": "9edb1ac6-af65-4738-a11b-1fd16a6e98a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[197  99]\n",
            " [143  61]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.67      0.62       296\n",
            "           1       0.38      0.30      0.34       204\n",
            "\n",
            "    accuracy                           0.52       500\n",
            "   macro avg       0.48      0.48      0.48       500\n",
            "weighted avg       0.50      0.52      0.50       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "sm_train_x, sm_train_y = smote.fit_resample(X_train, Y_train)\n",
        "xgb_model.fit(sm_train_x, sm_train_y)\n",
        "test_evals(xgb_model, X_test, Y_test, len(sm_train_x), 'XGBoost', 'SMOTE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PkMphqQy2zn",
        "outputId": "8890a02e-058b-409e-eb7e-b50dbff1fe5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[160 136]\n",
            " [110  94]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.54      0.57       296\n",
            "           1       0.41      0.46      0.43       204\n",
            "\n",
            "    accuracy                           0.51       500\n",
            "   macro avg       0.50      0.50      0.50       500\n",
            "weighted avg       0.52      0.51      0.51       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adasyn = ADASYN(random_state=42)\n",
        "ada_train_x, ada_train_y = adasyn.fit_resample(X_train, Y_train)\n",
        "xgb_model.fit(ada_train_x, ada_train_y)\n",
        "test_evals(xgb_model, X_test, Y_test, len(ada_train_x), 'XGBoost', 'ADASYN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iljHkqLI0bmj",
        "outputId": "49b585a3-99ac-4c0e-ca1a-2534281d6d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[157 139]\n",
            " [110  94]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.53      0.56       296\n",
            "           1       0.40      0.46      0.43       204\n",
            "\n",
            "    accuracy                           0.50       500\n",
            "   macro avg       0.50      0.50      0.49       500\n",
            "weighted avg       0.51      0.50      0.51       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smtomek = SMOTETomek(random_state=42)\n",
        "tom_train_x, tom_train_y = smtomek.fit_resample(X_train, Y_train)\n",
        "xgb_model.fit(tom_train_x, tom_train_y)\n",
        "test_evals(xgb_model, X_test, Y_test, len(tom_train_x), 'XGBoost', 'SMOTE + Tomek')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfbZjgXp2Y7l",
        "outputId": "7b7faa4b-888a-4e1b-d018-947d1a5a5f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[163 133]\n",
            " [108  96]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.55      0.57       296\n",
            "           1       0.42      0.47      0.44       204\n",
            "\n",
            "    accuracy                           0.52       500\n",
            "   macro avg       0.51      0.51      0.51       500\n",
            "weighted avg       0.53      0.52      0.52       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm_enn = SMOTEENN(random_state=42)\n",
        "enn_train_x, enn_train_y = sm_enn.fit_resample(X_train, Y_train)\n",
        "xgb_model.fit(enn_train_x, enn_train_y)\n",
        "\n",
        "test_evals(xgb_model, X_test, Y_test, len(enn_train_x), 'XGBoost', 'SMOTE + ENN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Y2QuXk26EZ",
        "outputId": "b7462ad3-9c2b-4c0d-a993-118274778e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[122 174]\n",
            " [ 82 122]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.41      0.49       296\n",
            "           1       0.41      0.60      0.49       204\n",
            "\n",
            "    accuracy                           0.49       500\n",
            "   macro avg       0.51      0.51      0.49       500\n",
            "weighted avg       0.52      0.49      0.49       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler(random_state=42)\n",
        "rus_train_x, rus_train_y = rus.fit_resample(X_train, Y_train)\n",
        "xgb_model.fit(rus_train_x, rus_train_y)\n",
        "\n",
        "test_evals(xgb_model, X_test, Y_test, len(rus_train_y), 'XGBoost', 'Random Undersampling')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rpqYuj43ak2",
        "outputId": "a4379ca4-cc88-476e-b2c8-88e8f5348518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[129 167]\n",
            " [ 91 113]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.44      0.50       296\n",
            "           1       0.40      0.55      0.47       204\n",
            "\n",
            "    accuracy                           0.48       500\n",
            "   macro avg       0.49      0.49      0.48       500\n",
            "weighted avg       0.51      0.48      0.49       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "ros_train_x, ros_train_y = ros.fit_resample(X_train, Y_train)\n",
        "xgb_model.fit(ros_train_x, ros_train_y)\n",
        "\n",
        "test_evals(xgb_model, X_test, Y_test, len(ros_train_y), 'XGBoost', 'Random Oversampling')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dH8JVUV32xJ",
        "outputId": "4e153ee0-00f3-483f-de67-c008ae01ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "============================================================\n",
            "[[198  98]\n",
            " [131  73]] \n",
            "\n",
            "Classification Report\n",
            "============================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       296\n",
            "           1       0.43      0.36      0.39       204\n",
            "\n",
            "    accuracy                           0.54       500\n",
            "   macro avg       0.51      0.51      0.51       500\n",
            "weighted avg       0.53      0.54      0.53       500\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost"
      ],
      "metadata": {
        "id": "It2i73hVXWBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost_model = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
        "\n",
        "eval_set = [(X_test, Y_test)]\n",
        "#train model\n",
        "adaboost_model.fit(X_train,\n",
        "      Y_train,\n",
        "      verbose=True)\n",
        "test_evals(adaboost_model, X_test, Y_test, len(data_df['text']), 'AdaBoost', 'No Aug')"
      ],
      "metadata": {
        "id": "d070ZZZ3XTMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smote = SMOTE(random_state=42)\n",
        "# sm_train_x, sm_train_y = smote.fit_resample(X_train, Y_train)\n",
        "adaboost_model.fit(sm_train_x, sm_train_y)\n",
        "test_evals(adaboost_model, X_test, Y_test, len(sm_train_x), 'AdaBoost', 'SMOTE')"
      ],
      "metadata": {
        "id": "HGe7Lwm6XYZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adasyn = ADASYN(random_state=42)\n",
        "# ada_train_x, ada_train_y = adasyn.fit_resample(X_train, Y_train)\n",
        "adaboost_model.fit(ada_train_x, ada_train_y)\n",
        "test_evals(adaboost_model, X_test, Y_test, len(ada_train_x), 'AdaBoost', 'ADASYN')"
      ],
      "metadata": {
        "id": "RVmcaeSRXiuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# smtomek = SMOTETomek(random_state=42)\n",
        "# tom_train_x, tom_train_y = smtomek.fit_resample(X_train, Y_train)\n",
        "adaboost_model.fit(tom_train_x, tom_train_y)\n",
        "test_evals(adaboost_model, X_test, Y_test, len(tom_train_x), 'AdaBoost', 'SMOTE + Tomek')"
      ],
      "metadata": {
        "id": "NWzymt2AXlWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sm_enn = SMOTEENN(random_state=42)\n",
        "# enn_train_x, enn_train_y = sm_enn.fit_resample(X_train, Y_train)\n",
        "adaboost_model.fit(enn_train_x, enn_train_y)\n",
        "\n",
        "test_evals(adaboost_model, X_test, Y_test, len(enn_train_x), 'AdaBoost', 'SMOTE + ENN')"
      ],
      "metadata": {
        "id": "EIyypokqXoOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rus = RandomUnderSampler(random_state=42)\n",
        "# rus_train_x, rus_train_y = rus.fit_resample(X_train, Y_train)\n",
        "adaboost_model.fit(rus_train_x, rus_train_y)\n",
        "\n",
        "test_evals(adaboost_model, X_test, Y_test, len(rus_train_y), 'AdaBoost', 'Random Undersampling')"
      ],
      "metadata": {
        "id": "_p-makvQXrZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ros = RandomOverSampler(random_state=42)\n",
        "# ros_train_x, ros_train_y = ros.fit_resample(X_train, Y_train)\n",
        "adaboost_model.fit(ros_train_x, ros_train_y)\n",
        "\n",
        "test_evals(adaboost_model, X_test, Y_test, len(ros_train_y), 'AdaBoost', 'Random Oversampling')"
      ],
      "metadata": {
        "id": "Rc7FYchTXtPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "7ehSMxnA5-it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#building the cnn in keras\n",
        "def build_cnn(sentence_length, word2vec_len):\n",
        "  model = Sequential([\n",
        "     Embedding(vocab_size, word2vec_len, input_length=sentence_length),\n",
        "     Conv1D(128, 5, activation='relu'),\n",
        "     GlobalMaxPooling1D(),\n",
        "     Dense(20, activation='relu'),\n",
        "     Dense(1, kernel_initializer='normal', activation='sigmoid')\n",
        "     ])\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "  return model"
      ],
      "metadata": {
        "id": "dHXke5mo6AJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "}\n",
        "scorers = {\n",
        "        'precision_score': make_scorer(precision_score),\n",
        "        'recall_score': make_scorer(recall_score),\n",
        "        'accuracy_score': make_scorer(accuracy_score)\n",
        "        }\n",
        "cnn_model = build_cnn(sentence_length, word2vec_len)\n",
        "# cnn_m = KerasClassifier(model = cnn_model, optimizer=\"adam\", epochs=5, verbose=0)\n",
        "# search = GridSearchCV(estimator=cnn_m, param_grid=params, cv=5, scoring = scorers,\n",
        "#                       refit='accuracy_score', return_train_score=True)\n",
        "cnn_model.fit(X_train, Y_train, epochs=5, verbose=True, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9tI0g306FgN",
        "outputId": "ef70b793-14c9-4b43-bf77-0a8d1011a35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "79/79 [==============================] - 13s 147ms/step - loss: 0.5576 - accuracy: 0.7175 - precision_6: 0.6375 - recall_6: 0.1360\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - 2s 25ms/step - loss: 0.3077 - accuracy: 0.8808 - precision_6: 0.8096 - recall_6: 0.7880\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - 2s 26ms/step - loss: 0.1173 - accuracy: 0.9532 - precision_6: 0.9451 - recall_6: 0.8960\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - 2s 26ms/step - loss: 0.0353 - accuracy: 0.9872 - precision_6: 0.9825 - recall_6: 0.9747\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - 2s 28ms/step - loss: 0.0170 - accuracy: 0.9924 - precision_6: 0.9854 - recall_6: 0.9893\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d7164593f40>"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions = cnn_model.predict(X_test)\n",
        "#accuracy_score(Y_test, predictions)\n",
        "evals(cnn_model, X_test, Y_test, len(X_test), 'CNN', 'No Aug')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPlrhNGp8Vi_",
        "outputId": "9b8356f5-d765-46f4-f459-1564f91b2e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 6ms/step - loss: 2.6972 - accuracy: 0.5580 - precision_6: 0.3924 - recall_6: 0.1520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_eval_df = pd.DataFrame({'model':models,\n",
        "                            'resample':resample,\n",
        "                            'accuracy': acc_list,\n",
        "                            'precision':precision_list,\n",
        "                            'recall':recall_list,\n",
        "                            'train length':train_length,\n",
        "                            'test_length':test_length})"
      ],
      "metadata": {
        "id": "HWNeuTdx6kaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_eval_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "aJjQtM-G4cHs",
        "outputId": "2aca584c-14f4-40f2-c886-bec0be406993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      model              resample  accuracy  precision    recall  \\\n",
              "0   XGBoost                No Aug     0.516   0.381250  0.299020   \n",
              "1   XGBoost                 SMOTE     0.508   0.408696  0.460784   \n",
              "2   XGBoost                ADASYN     0.502   0.403433  0.460784   \n",
              "3   XGBoost         SMOTE + Tomek     0.518   0.419214  0.470588   \n",
              "4   XGBoost           SMOTE + ENN     0.488   0.412162  0.598039   \n",
              "5   XGBoost  Random Undersampling     0.484   0.403571  0.553922   \n",
              "6   XGBoost   Random Oversampling     0.542   0.426901  0.357843   \n",
              "7   Bi-LSTM           SMOTE + ENN     0.592   0.000000  0.000000   \n",
              "8   Bi-LSTM                No Aug     0.746   0.750503  0.746000   \n",
              "9   Bi-LSTM                No Aug     0.996   0.000000  0.000000   \n",
              "10      CNN                No Aug     0.558   0.392405  0.151961   \n",
              "\n",
              "    train length  test_length  \n",
              "0           2499          500  \n",
              "1           3498          500  \n",
              "2           3445          500  \n",
              "3           3392          500  \n",
              "4           1714          500  \n",
              "5           1500          500  \n",
              "6           3498          500  \n",
              "7           1631          500  \n",
              "8           2500          500  \n",
              "9           2499          500  \n",
              "10           500          500  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2ba6f61-c7e4-4777-a91f-58369356871a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>resample</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>train length</th>\n",
              "      <th>test_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>0.516</td>\n",
              "      <td>0.381250</td>\n",
              "      <td>0.299020</td>\n",
              "      <td>2499</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>SMOTE</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.408696</td>\n",
              "      <td>0.460784</td>\n",
              "      <td>3498</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.403433</td>\n",
              "      <td>0.460784</td>\n",
              "      <td>3445</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>SMOTE + Tomek</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.419214</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>3392</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>SMOTE + ENN</td>\n",
              "      <td>0.488</td>\n",
              "      <td>0.412162</td>\n",
              "      <td>0.598039</td>\n",
              "      <td>1714</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>Random Undersampling</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.403571</td>\n",
              "      <td>0.553922</td>\n",
              "      <td>1500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>Random Oversampling</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.426901</td>\n",
              "      <td>0.357843</td>\n",
              "      <td>3498</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Bi-LSTM</td>\n",
              "      <td>SMOTE + ENN</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1631</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Bi-LSTM</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.750503</td>\n",
              "      <td>0.746000</td>\n",
              "      <td>2500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bi-LSTM</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>0.996</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2499</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CNN</td>\n",
              "      <td>No Aug</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.392405</td>\n",
              "      <td>0.151961</td>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2ba6f61-c7e4-4777-a91f-58369356871a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2ba6f61-c7e4-4777-a91f-58369356871a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2ba6f61-c7e4-4777-a91f-58369356871a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4ae6de65-7c12-41fd-961b-959d9466308e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ae6de65-7c12-41fd-961b-959d9466308e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4ae6de65-7c12-41fd-961b-959d9466308e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "clf_eval_df",
              "summary": "{\n  \"name\": \"clf_eval_df\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"XGBoost\",\n          \"Bi-LSTM\",\n          \"CNN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resample\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"No Aug\",\n          \"SMOTE\",\n          \"Random Undersampling\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.154637167173777,\n        \"min\": 0.484,\n        \"max\": 0.9959999918937683,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.484,\n          0.516,\n          0.9959999918937683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20734539151429723,\n        \"min\": 0.0,\n        \"max\": 0.7505030035972595,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7505030035972595,\n          0.40869565217391307,\n          0.4035714285714286\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24088535763756785,\n        \"min\": 0.0,\n        \"max\": 0.7459999918937683,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.7459999918937683,\n          0.46078431372549017,\n          0.35784313725490197\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 999,\n        \"min\": 500,\n        \"max\": 3498,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          2500,\n          3498,\n          1500\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 500,\n        \"max\": 500,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          500\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 340
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiLSTM"
      ],
      "metadata": {
        "id": "81hMgnuAYcGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_x_y(df, num_classes, word2vec_len, input_size, word2vec):\n",
        "\n",
        "  #read in lines\n",
        "  train_lines = df\n",
        "  num_lines = len(train_lines)\n",
        "\n",
        "  #initialize x and y matrix\n",
        "  x_matrix = None\n",
        "  y_matrix = None\n",
        "\n",
        "  try:\n",
        "    x_matrix = np.zeros((num_lines, input_size, word2vec_len))\n",
        "  except:\n",
        "    print(\"Error!\", num_lines, input_size, word2vec_len)\n",
        "  y_matrix = np.zeros((num_lines, num_classes))\n",
        "\n",
        "  #insert values\n",
        "  for i, line in enumerate(train_lines):\n",
        "    #parts = line[:-1].split('\\t')\n",
        "    label = int(train_lines['label'][i])\n",
        "    sentence = train_lines['text'][i]\n",
        "\n",
        "    #insert x\n",
        "    words = sentence.split(' ')\n",
        "    words = words[:x_matrix.shape[1]] #cut off if too long\n",
        "    for j, word in enumerate(words):\n",
        "      if word in word2vec:\n",
        "        x_matrix[i, j, :] = word2vec[word]\n",
        "\n",
        "    #insert y\n",
        "    y_matrix[i][label] = 1.0\n",
        "  return x_matrix, y_matrix"
      ],
      "metadata": {
        "id": "M8O8SLu-Gt6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_models(sentence_length, word2vec_len):\n",
        "\tmodel = Sequential([\n",
        "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(sentence_length, word2vec_len)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(32, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(20, activation='relu'),\n",
        "    Dense(2, activation='sigmoid')\n",
        "    ])\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\t#print(model.summary())\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "QzxhIzkTInjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = build_models(sentence_length, word2vec_len)"
      ],
      "metadata": {
        "id": "Z7vE1_EVSAed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "# data_train = '/content/drive/MyDrive/aathesis/data_clean.txt'\n",
        "# data_test = '/content/drive/MyDrive/aathesis/test_clean.txt'\n",
        "train_x, train_y = get_x_y(data_df, 2, word2vec_len, sentence_length, word_vectors)\n",
        "test_x, test_y = get_x_y(test_df, 2, word2vec_len, sentence_length, word_vectors)\n",
        "\n",
        "#implement early stopping\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
        "\n",
        "#train model\n",
        "model_lstm.fit(train_x, train_y, epochs=100, callbacks=callbacks,\n",
        "               validation_split=0.1,\n",
        "               batch_size=32,\n",
        "               shuffle=False,\n",
        "               verbose=1)"
      ],
      "metadata": {
        "id": "z9cHzExERalt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evals(model_lstm, test_x, test_y, len(train_x), 'Bi-LSTM', 'No Aug')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyYoAZweVrky",
        "outputId": "fe91f44c-1814-48dd-89db-3ddc82fb98fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 0.9960 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model\n",
        "y_preds = model_lstm.predict(test_x)\n",
        "test_y_cat = one_hot_to_categorical(test_y)\n",
        "y_pred_cat = one_hot_to_categorical(y_preds)\n",
        "acc = accuracy_score(test_y_cat, y_pred_cat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0SOZnijUF7i",
        "outputId": "6b3c29fb-561c-4d57-bd75-54509087c786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 7ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlgQdbJReExV",
        "outputId": "4292df8a-5f34-4280-98f4-9c1f0b867347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_train = np.array(train_x).reshape(train_x.shape[0], train_x.shape[1] * train_x.shape[2])\n",
        "#reshaped_test = np.array(enn_train_x).reshape(X_test.shape[0], X_test.shape[1], word2vec_len)"
      ],
      "metadata": {
        "id": "in7LGXXopl1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB1-tNhfXAYw",
        "outputId": "3da32dbe-d7d3-40c7-a4d5-d7d4afb166df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2499, 9000)"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_train = train_x[:, :, 0]"
      ],
      "metadata": {
        "id": "Lm-d2T72SzJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = train_x[:, :, 0]\n",
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8l5oTBrSN6W",
        "outputId": "7a13c522-2266-4e47-b3d0-cc7a8b424f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2499, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[0], X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMblZ8WiYLqV",
        "outputId": "1c865036-1020-4696-a370-c189eac98c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.83695924, -0.58585596,  0.85513383,  0.29589596,  0.65106684,\n",
              "         0.25178507,  1.57285666, -0.35270053,  0.00830237, -0.13255566,\n",
              "         0.37113538,  0.65869963, -0.13255566, -0.02887333, -0.83695924,\n",
              "        -0.22716631, -1.65890098, -2.18556809, -0.18538205, -1.02130592,\n",
              "        -1.65890098,  1.12810314, -0.89930028,  2.06105924, -1.3888917 ,\n",
              "        -0.14247988,  0.25021523,  0.        ,  0.93755132, -0.3380031 ]),\n",
              " array([ 553, 2020,  603,  115,  383,    6,   98,  316, 2021,  219,  316,\n",
              "        2953,  152,   23,    5,  266,  514,   52,    5,   21,  196,   50,\n",
              "           4,   45,  515, 2954,    1,  289,  347, 2955], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_train_y = one_hot_to_categorical(train_y)"
      ],
      "metadata": {
        "id": "_KwflK48RYB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### INPUT KE SMOTE\n",
        "X_train.shape, Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoHcGugPWaBP",
        "outputId": "c0169d56-94c9-4631-91c7-2e61bb581977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2499, 30), (2499,))"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smote_enn = SMOTEENN()\n",
        "smote_enn_x, smote_enn_y = smote_enn.fit_resample(result, Y_train)"
      ],
      "metadata": {
        "id": "1eVGZGbiBWh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_enn_x.shape, smote_enn_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vIGHALDXS7t",
        "outputId": "e5226ce9-e081-4661-cd10-7652181ec6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 30), (0,))"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### INPUT KE MODEL\n",
        "train_x.shape, train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8nmUWsgRaDg",
        "outputId": "c84e03fd-2aa3-4fe8-c7bd-9be19a53bf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2499, 30, 300), (2499, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_x_back = np.array(smote_enn_x).reshape(smote_enn_x.shape[0], 30, 300)"
      ],
      "metadata": {
        "id": "pMjhIgfdBkPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_matrix = np.zeros((smote_enn_x.shape[0], 2))\n",
        "for i, line in enumerate(smote_enn_y):\n",
        "  a = int(smote_enn_y[i])\n",
        "  if a == 0:\n",
        "    y_matrix[i][a] = 1\n",
        "res_y_back = y_matrix"
      ],
      "metadata": {
        "id": "MI5TZiirCq5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_enn_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERW7Bm8lPvRl",
        "outputId": "385df10b-158e-4f34-bc7d-c131007b3d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1673, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "model_lstm.fit(res_x_back, res_y_back, epochs=100, callbacks=callbacks,\n",
        "               validation_split=0.1,\n",
        "               batch_size=32,\n",
        "               shuffle=False,\n",
        "               verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "TuazcMRQ7QUm",
        "outputId": "78e90d43-0ebe-4e11-e0e7-2c35d1f9e058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_lstm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-739f18752d6b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_lstm.fit(res_x_back, res_y_back, epochs=100, callbacks=callbacks,\n\u001b[0m\u001b[1;32m      3\u001b[0m                \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_lstm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evals(model_lstm, test_x, test_y, len(res_x_back), 'Bi-LSTM', 'SMOTE + ENN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYO6wWSXKQy8",
        "outputId": "274683ad-e690-4182-ff27-a450194989b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 11ms/step - loss: 5.1219 - accuracy: 0.5920 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.key_to_index[\"syukur\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Ph6gZZt6d_",
        "outputId": "927cbed2-8c73-449b-c458-00fdd8ff7f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7458"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "B9g-fUTxfU-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create BiLSTM model\n",
        "def create_bilstm(sentence_length, word2vec_len):\n",
        "    model = Sequential()\n",
        "    # Input layer\n",
        "    model.add(Bidirectional(\n",
        "              LSTM(64, return_sequences=True),\n",
        "              input_shape=(sentence_length, word2vec_len)))\n",
        "    # Hidden layer\n",
        "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    #Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "sFNHxYekKrZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the gru in keras\n",
        "def build_gru(sentence_length, word2vec_len):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.GRU(64, input_shape=(sentence_length, word2vec_len)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(20))\n",
        "  model.add(layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "  return model"
      ],
      "metadata": {
        "id": "_D9Wte2bOh0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = build_gru(sentence_length, word2vec_len)"
      ],
      "metadata": {
        "id": "aI4KuR0Nf116"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_trains = X_train.reshape(-1, 2499, 30)\n",
        "x_trains.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_5PFznW0wF7",
        "outputId": "3278255d-7cc7-4683-e521-f97525a863b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2499, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in my_tokenizer.word_index.items():\n",
        "\tembedding_vector = word_vectors.key_to_index[word]\n",
        "\tif embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "MNxLGnsSvbcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = TokenizerTransformer()\n",
        "my_padder = PadSequencesTransformer(maxlen=30)\n",
        "my_model = KerasClassifier(\n",
        "               build_fn=create_model,\n",
        "               epochs=100,\n",
        "               embedding_input_dim=vocab_size,\n",
        "               embedding_output_dim=EMBEDDING_DIM,\n",
        "               embedding_weights=embedding_weights\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "              ('tokenizer', my_tokenizer),\n",
        "              ('padder', my_padder),\n",
        "              ('smote', SMOTE(random_state=42)),\n",
        "              ('model', my_model)\n",
        "])\n",
        "pipeline.fit(data_df['text'], data_df['label'])"
      ],
      "metadata": {
        "id": "bVqBWiu4Oc6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = TokenizerTransformer()\n",
        "my_padder = PadSequencesTransformer(maxlen=longest_text)\n",
        "my_model = KerasClassifier(\n",
        "  model=create_model,\n",
        "  epochs=200,\n",
        "  embedding_input_dim=vocab_size,\n",
        "  embedding_output_dim=EMBEDDING_DIM,\n",
        "  embedding_weights=embedding_weights,\n",
        "  batch_size=1024,\n",
        "  validation_split=0.1,\n",
        "  verbose=0)\n",
        "\n",
        "pipelines = Pipeline([\n",
        "  ('tokenizer', my_tokenizer),\n",
        "  ('padder', my_padder),\n",
        "  ('smote', SMOTE(random_state=42)),\n",
        "  ('model', my_model)\n",
        "])\n",
        "pipelines.fit(data_df['text'], data_df['label'])\n"
      ],
      "metadata": {
        "id": "whjIvjWCOzyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def evaluate():\n",
        "test_df = pd.read_csv('/content/increment_datasets_f2/test.txt', sep='\\t', header = None)\n",
        "test_df.columns = ['label', 'text']\n",
        "y_pred = pipeline.predict(test_df['text'])\n",
        "acc = accuracy_score(test_df['label'], y_pred)"
      ],
      "metadata": {
        "id": "80DHnjSGWPHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "KtWi24IFlQyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_smote = Pipeline([\n",
        "  ('tokenizer', my_tokenizer),\n",
        "  ('padder', my_padder),\n",
        "  ('model', my_model)\n",
        "])\n",
        "no_smote.fit(data_df['text'], data_df['label'])"
      ],
      "metadata": {
        "id": "N7HIVGceWC5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ada_tokenizer = TokenizerTransformer()\n",
        "ada_padder = PadSequencesTransformer(maxlen=30)\n",
        "ada_model = KerasClassifier(\n",
        "               build_fn=create_model,\n",
        "               epochs=200,\n",
        "               embedding_input_dim=vocab_size,\n",
        "               embedding_output_dim=EMBEDDING_DIM,\n",
        "               embedding_weights=embedding_weights\n",
        ")\n",
        "\n",
        "ada = Pipeline([\n",
        "              ('tokenizer', ada_tokenizer),\n",
        "              ('padder', ada_padder),\n",
        "              ('smote', ADASYN()),\n",
        "              ('model', ada_model)\n",
        "])\n",
        "ada.fit(data_df['text'], data_df['label'])"
      ],
      "metadata": {
        "id": "IHWMW0LTG_XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ada_tokenizer = TokenizerTransformer()\n",
        "ada_padder = PadSequencesTransformer(maxlen=30)\n",
        "ada_model = KerasClassifier(\n",
        "               build_fn=create_model,\n",
        "               epochs=200,\n",
        "               embedding_input_dim=vocab_size,\n",
        "               embedding_output_dim=EMBEDDING_DIM,\n",
        "               embedding_weights=embedding_weights\n",
        ")\n",
        "\n",
        "SMTomek = Pipeline([\n",
        "              ('tokenizer', ada_tokenizer),\n",
        "              ('padder', ada_padder),\n",
        "              ('SMOTETomek', SMOTETomek()),\n",
        "              ('model', ada_model)\n",
        "])\n",
        "SMTomek.fit(data_df['text'], data_df['label'])"
      ],
      "metadata": {
        "id": "VK0lpWgoMeo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ada_tokenizer = TokenizerTransformer()\n",
        "ada_padder = PadSequencesTransformer(maxlen=30)\n",
        "ada_model = KerasClassifier(\n",
        "               build_fn=create_model,\n",
        "               epochs=200,\n",
        "               embedding_input_dim=vocab_size,\n",
        "               embedding_output_dim=EMBEDDING_DIM,\n",
        "               embedding_weights=embedding_weights\n",
        ")\n",
        "\n",
        "SMENN = Pipeline([\n",
        "              ('tokenizer', ada_tokenizer),\n",
        "              ('padder', ada_padder),\n",
        "              ('SMOTEENN', SMOTEENN()),\n",
        "              ('model', ada_model)\n",
        "])\n",
        "SMENN.fit(data_df['text'], data_df['label'])"
      ],
      "metadata": {
        "id": "1fR9igOnRvGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zj1-kRyIBhTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = KeyedVectors.load(\"/content/idwiki_word2vec_300.model\", mmap='r')\n",
        "word_vectorss = wv.wv"
      ],
      "metadata": {
        "id": "HY4QcsxXHo7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-vF2TUryHyAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "predictions = xgb.predict(test_df['text'])\n",
        "accuracy_score(test_df['label'], predictions)"
      ],
      "metadata": {
        "id": "ESGKWJUfMBe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipeline.predict(test_df['text'])\n",
        "accuracy_score(test_df['label'], y_pred)"
      ],
      "metadata": {
        "id": "4eqZOhFM1nIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = no_smote.predict(test_df['text'])\n",
        "accuracy_score(test_df['label'], y_preds)"
      ],
      "metadata": {
        "id": "fHYEFI651nMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = ada.predict(test_df['text'])\n",
        "accuracy_score(test_df['label'], y_preds)"
      ],
      "metadata": {
        "id": "7RuHr3g6MORO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tomek = SMTomek.predict(test_df['text'])\n",
        "accuracy_score(test_df['label'], y_pred_tomek)"
      ],
      "metadata": {
        "id": "Ygq_4eeiRV__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_smoteenn = SMENN.predict(test_df['text'])\n",
        "accuracy_score(test_df['label'], y_pred_smoteenn)"
      ],
      "metadata": {
        "id": "lGqKyO8hUGft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(no_smote, '/content/drive/MyDrive/aathesis/save/no_smote.pkl')"
      ],
      "metadata": {
        "id": "fxC61TAJ1nPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(ada, '/content/drive/MyDrive/aathesis/save/ada.pkl')\n",
        "joblib.dump(SMTomek, '/content/drive/MyDrive/aathesis/save/SMTomek.pkl')\n",
        "joblib.dump(SMENN, '/content/drive/MyDrive/aathesis/save/SMENN.pkl')"
      ],
      "metadata": {
        "id": "VxxDCmp2X_yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = joblib.load(PIPE_PATH)"
      ],
      "metadata": {
        "id": "f7AqwhWS6pep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_df(txt_file):\n",
        "  data_df = pd.read_csv(txt_file, sep='\\t', header = None)\n",
        "  data_df.columns = ['label', 'text']\n",
        "  return data_df"
      ],
      "metadata": {
        "id": "UcUG_AxLmimv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(train_file, test_file, input_size, percent_dataset):\n",
        "\n",
        "  #initialize model\n",
        "  #model = build_cnn(input_size, word2vec_len, num_classes)\n",
        "\n",
        "  model = xgb.XGBClassifier(n_jobs=-1)\n",
        "\n",
        "  #load data\n",
        "  data_df = text_to_df(train_file)\n",
        "  test_df = text_to_df(test_file)\n",
        "\n",
        "  my_tokenizer = TokenizerTransformer()\n",
        "  my_padder = PadSequencesTransformer(maxlen=input_size)\n",
        "  my_tokenizer.fit(data_df['text'])\n",
        "  tokenized = my_tokenizer.transform(data_df['text'])\n",
        "  my_padder.fit(tokenized)\n",
        "  train_x = my_padder.transform(tokenized)\n",
        "  train_y = data_df['label']\n",
        "\n",
        "  test_tokenizer = TokenizerTransformer()\n",
        "  test_padder = PadSequencesTransformer(maxlen=input_size)\n",
        "  test_tokenizer.fit(test_df['text'])\n",
        "  test_tokenized = test_tokenizer.transform(test_df['text'])\n",
        "  test_padder.fit(test_tokenized)\n",
        "  test_x = test_padder.transform(test_tokenized)\n",
        "  test_y = test_df['label']\n",
        "\n",
        "  #smote = SMOTE()\n",
        "  #train_x, train_y = smote.fit_resample(X_train, train_y)\n",
        "  eval_set = [(test_x, test_y)]\n",
        "  #train model\n",
        "  model.fit(train_x,\n",
        "        train_y,\n",
        "        eval_metric=[\"error\", \"logloss\"],\n",
        "        eval_set=eval_set,\n",
        "        verbose=True)\n",
        "  #model.save('checkpoints/lol')\n",
        "  #model = load_model('checkpoints/lol')\n",
        "  #evaluate model\n",
        "  y_pred = model.predict(test_x)\n",
        "  acc = accuracy_score(test_y, y_pred)\n",
        "  results = model.evals_result()\n",
        "  #return the accuracy\n",
        "  #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
        "  return acc, results"
      ],
      "metadata": {
        "id": "l7NBMrbyhtAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_orig = '/content/drive/MyDrive/aathesis/increment_datasets_f2/pc/orig_full.txt'\n",
        "test_path = '/content/drive/MyDrive/aathesis/increment_datasets_f2/pc/test.txt'"
      ],
      "metadata": {
        "id": "dCUvFZzksnKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accs, results= run_model(train_orig, test_path, 50, 1)"
      ],
      "metadata": {
        "id": "LLSpzKqfk7Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHMHZq0PRRZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_tokenizer = TokenizerTransformer()\n",
        "my_padder = PadSequencesTransformer(maxlen=30)\n",
        "my_model = KerasClassifier(\n",
        "  model=create_model,\n",
        "  epochs=200,\n",
        "  embedding_input_dim=vocab_size,\n",
        "  embedding_output_dim=EMBEDDING_DIM,\n",
        "  embedding_weights=embedding_weights,\n",
        "  batch_size=1024,\n",
        "  validation_split=0.1,\n",
        "  verbose=True)\n"
      ],
      "metadata": {
        "id": "Ov8-qxJNydX3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "ab1bc86e-8d9c-4969-e711-077290707a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-515d84ec55db>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_padder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPadSequencesTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m my_model = KerasClassifier(\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0membedding_input_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = build_cnn(30, 300, 2)"
      ],
      "metadata": {
        "id": "xMQgl02RTxez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models.fit(train_x, train_y, epochs = 100, verbose = True)"
      ],
      "metadata": {
        "id": "xrnhJJsgU-6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models.predict(test_x)"
      ],
      "metadata": {
        "id": "I1BfhAXUVGt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of unique words in dictionary=\",\n",
        "      len(my_tokenizer.word_index))\n",
        "print(\"Dictionary is = \", my_tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4GD-UkGOckb",
        "outputId": "5c698679-12ee-4c01-d242-9d6b09a23e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 0\n",
            "Dictionary is =  {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index,\n",
        "                        embedding_dim):\n",
        "  voc_size = len(word_index) + 1\n",
        "\n",
        "  # Adding again 1 because of reserved 0 index\n",
        "  embedding_matrix_vocab = np.zeros((voc_size,\n",
        "                                      embedding_dim))\n",
        "\n",
        "  with open(filepath, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word in word_index:\n",
        "        idx = word_index[word]\n",
        "        embedding_matrix_vocab[idx] = np.array(\n",
        "            vector, dtype=np.float32)[:embedding_dim]\n",
        "  return embedding_matrix_vocab\n",
        "\n",
        "\n",
        "# matrix for vocab: word_index\n",
        "embedding_dim = 300\n",
        "embedding_matrix_vocab = embedding_for_vocab(\n",
        "    '/content/drive/MyDrive/aathesis/glove.6B.300d.txt', my_tokenizer.word_index,\n",
        "  embedding_dim)\n",
        "\n",
        "# print(\"Dense vector for first word is => \",\n",
        "#       embedding_matrix_vocab[1])"
      ],
      "metadata": {
        "id": "_oVNIO_zMcCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_vocab.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Keh-nbiOOHLw",
        "outputId": "8454f407-43ec-436e-b1f0-f423ca35a3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#get full image paths\n",
        "def get_txt_paths(folder):\n",
        "    txt_paths = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and '.txt' in f]\n",
        "    if join(folder, '.DS_Store') in txt_paths:\n",
        "        txt_paths.remove(join(folder, '.DS_Store'))\n",
        "    txt_paths = sorted(txt_paths)\n",
        "    return txt_paths\n",
        "\n",
        "#get subfolders\n",
        "def get_subfolder_paths(folder):\n",
        "    subfolder_paths = [join(folder, f) for f in listdir(folder) if (isdir(join(folder, f)) and '.DS_Store' not in f)]\n",
        "    if join(folder, '.DS_Store') in subfolder_paths:\n",
        "        subfolder_paths.remove(join(folder, '.DS_Store'))\n",
        "    subfolder_paths = sorted(subfolder_paths)\n",
        "    return subfolder_paths\n",
        "\n",
        "#get all image paths\n",
        "def get_all_txt_paths(master_folder):\n",
        "\n",
        "    all_paths = []\n",
        "    subfolders = get_subfolder_paths(master_folder)\n",
        "    if len(subfolders) > 1:\n",
        "        for subfolder in subfolders:\n",
        "            all_paths += get_txt_paths(subfolder)\n",
        "    else:\n",
        "        all_paths = get_txt_paths(master_folder)\n",
        "    return all_paths"
      ],
      "metadata": {
        "id": "J32hzGU3QlL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the pickle file for the word2vec so you don't have to load the entire huge file each time\n",
        "def gen_vocab_dicts(folder, huge_word2vec):\n",
        "  vocab = set()\n",
        "  #text_embeddings = open(huge_word2vec, 'r').readlines()\n",
        "  wv = KeyedVectors.load(\"/content/idwiki_word2vec_300.model\", mmap='r')\n",
        "  text_embeddings = wv.wv\n",
        "  word2vec = {}\n",
        "\n",
        "  #get all the vocab\n",
        "  all_txt_paths = get_all_txt_paths(folder)\n",
        "  print(all_txt_paths)\n",
        "\n",
        "  #loop through each text file\n",
        "  for txt_path in all_txt_paths:\n",
        "\n",
        "  # get all the words\n",
        "    try:\n",
        "      all_lines = open(txt_path, \"r\").readlines()\n",
        "      for line in all_lines:\n",
        "        words = line[:-1].split(' ')\n",
        "        for word in words:\n",
        "          vocab.add(word)\n",
        "    except:\n",
        "      print(txt_path, \"has an error\")\n",
        "\n",
        "    print(len(vocab), \"unique words found\")\n",
        "\n",
        "  # load the word embeddings, and only add the word to the dictionary if we need it\n",
        "  for line in text_embeddings:\n",
        "    print(line)\n",
        "    items = line.split(' ')\n",
        "    word = items[0]\n",
        "    if word in vocab:\n",
        "      vec = items[1:]\n",
        "      word2vec[word] = np.asarray(vec, dtype = 'float32')\n",
        "  print(len(word2vec), \"matches between unique words and word2vec dictionary\")\n",
        "  return word2vec\n",
        "\n",
        "  # pickle.dump(word2vec, open(output_pickle_path, 'wb'))\n",
        "  # print(\"dictionaries outputted to\", output_pickle_path)\n"
      ],
      "metadata": {
        "id": "wUeZASl3Jx8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wo2vec = gen_vocab_dicts('/content/drive/MyDrive/aathesis',\n",
        "                        '/content/drive/MyDrive/aathesis/glove.6B.300d.txt')"
      ],
      "metadata": {
        "id": "bi1DAT0VRQn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec = gen_vocab_dicts('/content/drive/MyDrive/aathesis',\n",
        "                        '/content/drive/MyDrive/aathesis/glove.6B.300d.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3uzX6XMQP3k",
        "outputId": "6243786d-4992-4118-fa31-0692ce44f95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/aathesis/increment_datasets_f2/test.txt']\n",
            "3025 unique words found\n",
            "1176 matches between unique words and word2vec dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(w2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kraKfEy6Qz0j",
        "outputId": "0e819c82-df40-45a0-e354-fce7e4452c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1176"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_net():\n",
        "  train_comments = data_df['text']\n",
        "  train_labels = data_df['label']\n",
        "  test_comments = test_df['text']\n",
        "  test_labels = test_df['label']\n",
        "\n",
        "  max_words = 30\n",
        "  tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "  tokenize.fit_on_texts(train_comments)\n",
        "\n",
        "  x_tra = tokenize.texts_to_matrix(train_comments)\n",
        "  x_tes = tokenize.texts_to_matrix(test_comments)\n",
        "\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(train_labels)\n",
        "  y_tra = encoder.transform(train_labels)\n",
        "  y_tes = encoder.transform(test_labels)\n",
        "  print(y_tra.shape, y_tes.shape)\n",
        "\n",
        "  num_classes = np.max(y_tra) + 1\n",
        "  y_tra = utils.to_categorical(y_tra, num_classes)\n",
        "  y_tes = utils.to_categorical(y_tes, num_classes)\n",
        "  print(y_tra.shape)\n",
        "\n",
        "  batch_size = 32\n",
        "  epochs = 1\n",
        "  model = Sequential([\n",
        "     Embedding(vocab_size, 300, input_length=30),\n",
        "     Bidirectional(LSTM(64, return_sequences=True)),\n",
        "     Dropout(0.5),\n",
        "     Bidirectional(LSTM(32, return_sequences=False)),\n",
        "     Dropout(0.5),\n",
        "     Dense(20, activation='relu'),\n",
        "     Dense(2, activation='sigmoid')\n",
        "     ])\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x_tra, y_tra,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      verbose=1,\n",
        "                      validation_split=0.3)\n",
        "\n",
        "  score = model.evaluate(x_tes, y_tes,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "  return score[1]"
      ],
      "metadata": {
        "id": "0d48E1SdcrIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net()"
      ],
      "metadata": {
        "id": "-diOXgbRdZPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da4c1ca-9a21-4cd9-f36f-053733d32645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0.]\n",
            "(2499,) (500,)\n",
            "2\n",
            "(2499, 2) (500, 2)\n",
            "55/55 [==============================] - 11s 71ms/step - loss: 0.6222 - accuracy: 0.6878 - val_loss: 0.5539 - val_accuracy: 0.6987\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6836 - accuracy: 0.5920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5920000076293945"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save the iris classification model as a pickle file\n",
        "model_pkl_file = \"iris_classifier_model.pkl\"\n",
        "\n",
        "with open(model_pkl_file, 'wb') as file:\n",
        "  pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "ao6E8eRaq9nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from pickle file\n",
        "with open(model_pkl_file, 'rb') as file:\n",
        "  model = pickle.load(file)"
      ],
      "metadata": {
        "id": "efGOQxpOrCBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code for Glove word embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "x = {'text', 'the', 'leader', 'prime',\n",
        "     'natural', 'language'}\n",
        "\n",
        "# create the dict.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# number of unique words in dict.\n",
        "print(\"Number of unique words in dictionary=\",\n",
        "      len(tokenizer.word_index))\n",
        "print(\"Dictionary is = \", tokenizer.word_index)\n",
        "\n",
        "# download glove and unzip it in Notebook.\n",
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove*.zip\n",
        "\n",
        "# vocab: 'the': 1, mapping of words with\n",
        "# integers in seq. 1,2,3..\n",
        "# embedding: 1->dense vector\n",
        "def embedding_for_vocab(filepath, word_index,\n",
        "                        embedding_dim):\n",
        "    vocab_size = len(word_index) + 1\n",
        "\n",
        "    # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
        "                                       embedding_dim))\n",
        "\n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix_vocab[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix_vocab\n",
        "\n",
        "\n",
        "# matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab(\n",
        "    '../glove.6B.50d.txt', tokenizer.word_index,\n",
        "  embedding_dim)\n",
        "\n",
        "print(\"Dense vector for first word is => \",\n",
        "      embedding_matrix_vocab[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "afm9h0EsZhQw",
        "outputId": "501d05de-ce14-4f2e-df5c-bdb6f9c6df8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 6\n",
            "Dictionary is =  {'text': 1, 'natural': 2, 'leader': 3, 'the': 4, 'language': 5, 'prime': 6}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../glove.6B.50d.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-1bd1389ff3cf>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# matrix for vocab: word_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m embedding_matrix_vocab = embedding_for_vocab(\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;34m'../glove.6B.50d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   embedding_dim)\n",
            "\u001b[0;32m<ipython-input-94-1bd1389ff3cf>\u001b[0m in \u001b[0;36membedding_for_vocab\u001b[0;34m(filepath, word_index, embedding_dim)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                        embedding_dim))\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../glove.6B.50d.txt'"
          ]
        }
      ]
    }
  ]
}